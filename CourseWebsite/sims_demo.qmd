---
title: "Slendr simulations"
author: "Mark Ravinet"
execute:
  output: false
  eval: false
format: 
  html: default
  pdf: default
---

## Introduction - slendr

[`slendr`](https://www.slendr.net/) is a recently developed and extensively well-managed R package that acts as a front end for demographic simulations with both `msprime` and `SLiM`. It massively simplifies the process of running these programs, making them easy to interface with via `R` - so if you're not great at `python` (like me!) you can easily use `R` instead. It is also very flexible and because it is built on top of the extremely efficient simulation programs, it is very fast and lightweight.

Much of the information in this tutorial is based on the original `slendr`\](https://www.slendr.net/) tutorial docs - I would strongly recommend referring to these as they are the definitive source for learning how to start to using the package.

## Setting up

First we need to load the packages we will need for this session. They have already been installed, so this will not take long.

```{r }
library(slendr)
library(tidyverse)
```

Note that if you install `slendr` on your own machine, you will need it to setup all the back-end it uses to interact with python etc. We will not do this today because it is already configured for you. However to ensure you know how to do this, you simply run

```{r }
setup_env()
```

Even if `slendr` is setup, it is always a good idea to initiate the environment, to ensure everything is working properly ahead of analyses. Do this like so:

```{r }
init_env()
```

Now we're ready to begin simulating!

## A simple non-spatial simulation with two populations

`slendr` allows you to combine some simple and logically named R functions into a complex demographic model. The easiest way to learn how to do this is to simply use these functions! Here we will simulate two populations. The second splits from the first 30,000 generations ago.

```{r  }
# Population a with 30,000 individuals, arising 50000 generations ago
a <- population("a", time = 50000, N = 30000)

# Population b with 15,000 individuals, arising 30,000 generations ago
b <- population("b", parent = a, time = 30000, N = 15000)
```

Be sure to check what these populations look like in the R environment. Just call them as objects to see.

Since we are keeping this model very simple in order to learn, all we need to do next is compile the model - all components are combined (i.e. pop size etc) into a single R object.

```{r  }
model <- compile_model(
  populations = list(a, b),
  generation_time = 1,
  direction = "backward",
)
```

Note that here we specify the direction of this model is "backwards" - i.e. we are performing a coalescent (backwards-in-time) simulation.

One really nice feature of `slendr` is that it allows you to plot the model to ensure you have specified it correctly.

```{r  }
plot_model(model)
```

If everything looks as it should, we are ready to begin simulating.

### Running a simulation

Next up, we need to set our sampling scheme. Here we will take 30 individuals from each of our two populations:

```{r  }
samples <- schedule_sampling(model, times = 0, list(a, 30), list(b, 30))
```

Note that this function is called `schedule_sampling` - this is because it allows you to set the timing of sampling (i.e. you can sample at different temporal points across the model) and also the location - actually spatially if you wish. But more on that later.

With this setup, we can run the simulation. We will use `msprime` because this is a coalescent simulation over many thousands of generations and therefore it is much more efficient and fast.

```{r  }
ts <- msprime(model, samples = samples, sequence_length = 1000, recombination_rate = 0)
```

This should only take a few seconds. Note we have simulated a 1000 bp seuqence here with no recombination, but we could easily alter this if we wanted to. Next we should take a look at the simulation output.

```{r  }
ts 
```

So far this doesn't show a great deal, other than the fact that it is a tree-sequence stored temporarily on our computer. To get at what the simulation actually shows, we need to process it.

### Processing simulations - a simple example

Right now our simulation output is a tree sequence of 60 individuals, 30 from population a and 30 from population b. But we might want to calculate some population genetic statistics on this dataset. So here work towards calculating *F*~ST~.

First of all, we need to set up our populations - i.e. let the pipeline know which individuals are in each population. This is easy enough to do with built in functions and some `tidyR`.

```{r  }
a_pop <- ts_samples(ts) %>% filter(pop == a) %>% .$name
b_pop <- ts_samples(ts) %>% filter(pop == b) %>% .$name
```

Next we need to add mutations to our tree sequence. One of the reasons msprime is so fast is that it records only the tree sequence or genealogy - mutations can be added after the fact in order to ensure the simulation is extremely efficient. Here we will add simulations using a basic SNP mutation rate.

```{r  }
ts_m <- ts_mutate(ts, mutation_rate = 1*10^-4)
```

Now with mutations added and our populations defined, we can use the `tskit` functions (all denoted in the package by starting with `ts_`) to calculate *F*~ST~. This is extremly fast and there are a large number of different functions to calculate different statistics.

```{r  }
ts_fst(ts_m, sample_sets = list(a = a_pop, b = b_pop))
```

We can also calculate diversity using the `ts_diversity` function - **nb** this is per site - to get `pi` for the sequence, you would need to divide this value by the length (1000 in this case).

```{r  }
ts_diversity(ts_m, sample_sets = list(a = a_pop, b = b_pop),
             mode = "site")
```

So there we have it - two population genetic statistic estimates for a simple 2-population model, all calculated in a single R framework at high speed. If we placed all the commands we used together in an single R script, we could have run this all in seconds, using very little hard drive space.

## A simple simulation pipeline

Running a single simulation is useful for learning but it isn't that helpful as a standalone tool. Instead, we can combine the code we've learned to examine how the variaton in population parameters might influence the statistics we calculate. Here we will run the model we created above for different population sizes for population B (from 1000 to 15000) - how does it alter our estimate of *F*~ST~?

This code might look complex - but it is almost everything we covered above!

```{r  }
# set our sequence to simulate across
pop_sizes <- seq(1000, 15000, by = 1000)

# run our model within an sapply command
fst_i <- sapply(pop_sizes, function(x){
  # set the sampling scheme
  samples <- schedule_sampling(model, times = 0, list(a, 30), list(b, 30))
  # Population a
  a <- population("a", time = 50000, N = 30000)
  # Population b 
  b <- population("b", parent = a, time = 30000, N = x)
  
  # compile model
  model <- compile_model(
    populations = list(a, b),
    generation_time = 1,
    direction = "backward",
  )
  # run the model
  ts <- msprime(model, samples = samples, sequence_length = 1000, recombination_rate = 0)
  # add the mutations
  ts_m <- ts_mutate(ts, mutation_rate = 1*10^-4)
  # set the populations
  a_pop <- ts_samples(ts) %>% filter(pop == a) %>% .$name
  b_pop <- ts_samples(ts) %>% filter(pop == b) %>% .$name
  # calculate fst
  y <- ts_fst(ts_m, sample_sets = list(a = a_pop, b = b_pop))
  y$Fst
})
```

We can then combine our results into a `data.frame` and plot the results using `ggplot` to see how they vary across the parameters.

```{r  }
# create a data.frame or tibble
sims <- as_tibble(data.frame(pop_sizes = pop_sizes, fst = fst_i))

# plot the output
ggplot(sims, aes(pop_sizes, fst)) + geom_point() + geom_line()
```

Quite clearly from this simple simulation we can see that as the populations size of B increases, so does the magnitude of *F*~ST~ between the two populations.

### Adding to the model: gene flow

The model we have been using so far is very simple - it is basically a 2-deme isolation model. But what if we want to add gene flow between our two populations? We can do this very easily using the `gene_flow` function.

```{r  }
gf <-
  list(gene_flow(from = a, to = b, rate = 0.2, start = 10000, end = 0),
       gene_flow(from = b, to = a, rate = 0.2, start = 10000, end = 0)
  )
```

Here we have ensured that we are simulating gene flow that is symmetric between our two populations at a rate of 0.2 between populations. This starts 10,000 generations in the past and continues until the present day. We have defined these two events in and combined them into a `list` called `gf`

Next we compile our model, but this time include a gene flow term.

```{r  }
model <- compile_model(
  populations = list(a, b),
  gene_flow = gf,
  generation_time = 1,
  direction = "backward",
)
```

And as before, we can plot the model to see what it looks like:

```{r  }
plot_model(model)
```

As before, we can run our model and see how it alters our estimates of *F*~ST~ and diversity. We need to rerun our model, populate it with mutations, define our populations and then calculate the statistics.

```{r  }
  # run the model
  ts <- msprime(model, samples = samples, sequence_length = 1000, recombination_rate = 0)
  # add the mutations
  ts_m <- ts_mutate(ts, mutation_rate = 1*10^-4)
  # set the populations
  a_pop <- ts_samples(ts) %>% filter(pop == a) %>% .$name
  b_pop <- ts_samples(ts) %>% filter(pop == b) %>% .$name
  # calculate fst
  ts_fst(ts_m, sample_sets = list(a = a_pop, b = b_pop))
  # calculate diversity
  ts_diversity(ts_m, sample_sets = list(a = a_pop, b = b_pop),
             mode = "site")
```

Yet again, a single value is interesting but it doesn't tell us too much. So we will do what we did previously - rerunning our model but altering the population size of population B. **However**, this time we will see how gene flow influences our inference!

```{r  }
# set our sequence to simulate across
pop_sizes <- seq(1000, 15000, by = 1000)

# run our model within an sapply command
fst_g <- sapply(pop_sizes, function(x){
  # set the sampling scheme
  samples <- schedule_sampling(model, times = 0, list(a, 30), list(b, 30))
  # Population a
  a <- population("a", time = 50000, N = 30000)
  # Population b 
  b <- population("b", parent = a, time = 30000, N = x)
  
  # compile model - note the inclusion of gene flow
 model <- compile_model(
  populations = list(a, b),
  gene_flow = gf,
  generation_time = 1,
  direction = "backward",
)
 
  # run the model
  ts <- msprime(model, samples = samples, sequence_length = 1000, recombination_rate = 0)
  # add the mutations
  ts_m <- ts_mutate(ts, mutation_rate = 1*10^-4)
  # set the populations
  a_pop <- ts_samples(ts) %>% filter(pop == a) %>% .$name
  b_pop <- ts_samples(ts) %>% filter(pop == b) %>% .$name
  # calculate fst
  y <- ts_fst(ts_m, sample_sets = list(a = a_pop, b = b_pop))
  y$Fst
})
```

Now we can add the output of the simulations from this run to our previous simulations (those without gene flow) and see the difference

```{r  }
# combine everything into a tibble
sims <- as_tibble(data.frame(sims, fst_g))
# alter names! you will see why shortly 
colnames(sims) <- c("pop_sizes", "isolation", "gene_flow")
# pivot to allow easy plotting
sims_p <- pivot_longer(sims, -pop_sizes, names_to = "model", values_to = "fst")
# plot the output
ggplot(sims_p, aes(pop_sizes, fst, colour = model)) + geom_point() + geom_line()
```

This shows that changing the population size of b has the same result in both models (i.e. *F*~ST~ decreases with increasing pop size) but that *F*~ST~ is lower in the gene flow model - as we would expect!

### Adding to the model: resizing a population

As well as gene flow events, we can also resize populations so that they experience bottlenecks or growth over time. Yet again, `slendr` makes this very straightforward. All we need to do is pipe our population declaration to a `resize` function when we declare populations.

```{r  }
# Population a with 30,000 individuals, arising 50000 generations ago
a <- population("a", time = 50000, N = 30000)

# Population b with 15,000 individuals, arising 30,000 generations ago
b <- population("b", parent = a, time = 30000, N = 15000) %>%
  resize(N = 2000, how = "step", time = 5000, end = 0)
```

Remember because we are working with coalescent models, we are working backwards in time. So here we set population b to start with a population size of 2000 at time 0 and this then increases to the ancestral population size 5000 generations in the past. Importantly we set the `how` for this resize to `step` - i.e. it will just change suddenly.

Then we need to just declare our model again. For simplicity here, we'll do this without gene flow.

```{r  }
model <- compile_model(
  populations = list(a, b),
  generation_time = 1,
  direction = "backward",
)
```

Then we can plot it to see how this looks!

```{r  }
plot_model(model)
```

And what if we had set this to happen exponentially, rather than a sudden step?

```{r  }
# Population a with 30,000 individuals, arising 50000 generations ago
a <- population("a", time = 50000, N = 30000)

# Population b with 15,000 individuals, arising 30,000 generations ago
b <- population("b", parent = a, time = 30000, N = 15000) %>%
  resize(N = 2000, how = "exponential", time = 5000, end = 0)

# recompile the model
model <- compile_model(
  populations = list(a, b),
  generation_time = 1,
  direction = "backward",
)

# plot the model
plot_model(model)
```

We won't include the resizing in our simulation pipeline above because next we will try to develop a simple spatial simulation - this will be a very useful tool for landscape genomics!

## Spatial models

So far, our models have all been relatively basic with no spatial context. But what if adding a spatial context helped make them more realistic? This is a very difficult and complex topic but it is the main motivation behind the development of `slendr` - again another reason to refer to [its excellent and extensive website](https://www.slendr.net/).

### Setting up the spatial context

We will try to take our basic model and put it in a spatial context in order to get a flavour of the kind of things you can do with `slendr`. The very first thing we will do is define a map that we will use - we use the `world` function to do this - and we simply set the longitude (`xrange`) and latitude (`yrange`) to do this.

```{r }
map <- world(
  xrange = c(-13, 70), # min-max longitude
  yrange = c(18, 65),  # min-max latitude
  crs = "EPSG:3035"    # coordinate reference system (CRS) for West Eurasia
)
```

With this set, we can then plot the map using the `plot_map` function from `slendr` to make an easy map as a background for simulations.

```{r }
plot_map(map)
```

Next we will create two regions on our map - one over the UK, the other over Europe.

```{r }
# anatolia
anatolia <- region(
  "Anatolia", map,
  polygon = list(c(28, 35), c(40, 35), c(42, 40),
                 c(30, 43), c(27, 40), c(25, 38))
)
# europe
europe <- region(
  "Europe", map,
  polygon = list(
    c(-10, 35), c(-5, 36), c(10, 38), c(20, 35), c(23, 35),
    c(30, 45), c(20, 52), c(0, 50), c(-10, 48)
  )
)
plot_map(anatolia, europe)
```

So here we have a map with two polygons imposed on the top that define regions. With this spatial structure set up, we can now start to build a model that is anchored in this geographical context.

### Building a spatial model

As with our simpler, non-spatial models, we can start to specify our model using the same functions as previously - i.e. `population`, except this time we actually incorporate the map data.

```{r }
# european population
eur <- population(
  name = "eur", time = 6000, N = 2000,
  polygon = europe, map = map
)
# check it by plotting!
plot_map(eur)
# anatolian population
ana <- population( # Anatolian pop
  name = "ana", time = 6000, N = 3000,
  center = c(34, 38), radius = 500e3, polygon = anatolia, map = map
) %>%
  expand_range( # expand the range by 2.500 km
    by = 2500e3, start = 5000, end = 3000, overlap = 0.5,
    polygon = join(europe, anatolia)
  )
```

Note that the arguments `map` and `polygon` allow us to specify the map and polygon we have already defined - it is these arguments which give our population its spatial rooting.

With this done, we can now replot these polygons and you will see we have defined the ranges of the populations within the polygons - here they are explicitly bounded to the landscape. We will see why this is important shortly.

```{r }
plot_map(ana)
# plot both together
plot_map(eur, ana) # showing an expansion into europe
```

With our populations defined, we can also set out some gene flow events. We will do this exactly the same way we did with our non-spatial model earlier.

```{r  }
# this will not work
gf <- gene_flow(from = ana, to = eur, rate = 0.1, start = 5000, end = 4000, overlap = T)
# this will 
gf <- gene_flow(from = ana, to = eur, rate = 0.1, start = 3000, end = 2000, overlap = T)

```

The main difference here however is that we have now added an `overlap` argument. This is basically a requirement that populations must spatially overlap in order to to exchange genes.

With this done, we can then compile our model. The principle here is the same as with our non-spatial model but with some additional arguments. We will learn about these after we have run the command. Also, ensure population names are correct!

```{r  }
# compile model
model_dir <- paste0(tempfile(), "_tutorial-model")


model <- compile_model(
  populations = list(eur, ana), # populations defined above
  gene_flow = gf, # gene-flow events defined above
  generation_time = 30,
  resolution = 100e3, # resolution in meters per pixel
  competition = 130e3, mating = 100e3, # spatial interaction in SLiM
  dispersal = 700e3, # how far will offspring end up from their parents
  path = model_dir
)

plot_model(model, proportions = T) # to check
```

So what additional arguments have we added here that differs from our previous model compliation in the non-spatial examples?

-   Firstly we have the `resolution` argument. This is simply the resolution of the map to simulate on and how much a single pixel represents. Here we have set it to 100,000 units.
-   Next we have `competition` - this is the maximum distance between two individuals where they can influence each others fitness via competition. Here it is set to 130,000 which basically menas individuals influence each other only if the occur right next to one another on the map.
-   We also have `mating` - this sets the mating choice distance, i.e. the maximum distance an individual will find a mate over; set to 100,000 here, it means individuals look for mates in close proximity.
-   Last we have `dispersal` which is fairly self-explanatory as dispersal distance. In the context of our model, it determines how far an individual can move before contributing to next generation.

Note that we also specify a `path` to set a model directory, just so we can look at the files SLiM writes to the directory should we need to.

Finally we use `plot_model` to make sure the model is doing what we expect. If we are satisfied with this, we can now run it using `slim`. Note that this is a large difference from our non-spatial models which used `msprime`. SliM is a forward in time simulator which allows it to incorporate selection and spatial dynamics. [It is extremely powerful](https://messerlab.org/slim/) and I would strongly recommend you investigate it in more detail!

However one disadvantage of SLiM is that it is slower than `msprime` - this menas our simulation will take a bit longer to complete... but not too long!

```{r }
# set locations file
locations_file <- tempfile(fileext = ".gz")
# run the simulations
ts <- slim(model, sequence_length = 1000, recombination_rate = 0, method = "batch",
           locations = locations_file)
# look at the tree sequence
ts
```

And with that, we're done. Hopefully this has given you a taste of what is possible with `slendr`. There is so much more you could do and many ways to extend and expand these models. It is well worth spending some time with this excellent R package!

### Optional extra - animating your model

This actually doesn't work on our RStudio Server **but** it should work locally. This is just an optional example that allows you to see some of the ways `slendr` allows you to explore your simulations and models.

```{r }
# animate the model
animate_model(model = model, file = locations_file, steps = 50, width = 700, height = 400)
```

# fastsimcoal

Firstly before proceeding I want make it clear that this tutorial was originally designed by my colleague and friend, [Joana Meier](https://www.sanger.ac.uk/person/meier-joana/) and has been adapted from [here](https://speciationgenomics.github.io/fastsimcoal2/) for this course. The depth of this tutorial would not have been possible without her input!

`fastsimcoal2` is an extremely flexible demographic modelling software developed by [Laurent Excoffier and his group at University of Bern](http://www.cmpg.iee.unibe.ch/about_us/team/researchers/prof_dr_excoffier_laurent/index_eng.html). It uses the site frequency spectrum (SFS) to fit model parameters to the observed data by performing coalescent simulations. You can find the manual and more information [here](http://cmpg.unibe.ch/software/fastsimcoal2/).

It is worth noting that this is a complex piece of software to run and it takes a lot of time to master. However it is really worth taking your time with it and exploring the manual. It is extremely well documented (which is rare for a program like this).

### Preparing the input files

To run `fastsimcoal2` we need three input files which are all named in a consistent way. They are all just plain text files.

-   observed SFS - `${PREFIX}_jointDAFpop1_0.obs`
-   template file defining the demographic model - `${PREFIX}.tpl`
-   estimation file defining the parameters - `${PREFIX}.est`

### Observed SFS

The observed SFS can be a derived SFS (i.e. also known as DAF or an unfolded SFS) if the ancestral state is unknown or a minor allele frequency SFS (i.e. MAF or folded SFS) if you do know the ancestral state. `fastsimcoal2` will identify the kind of SFS by its name suffix and the command flag `-m` or `-d`. For a single population, it expects the name `${PREFIX}_DAFpop0.obs` or `${PREFIX}_MAFpop0.obs`.

For two populations, the file names should end with `jointDAFpop1_0.obs` or `jointMAFpop1_0.obs`. If more than two observed populations are used, pairwise MAFs or DAFs can be provided following the naming scheme above or a multidimensional SFS can be used ending with `_DSFS.obs` or `_MSFS.obs` and specifying the `-multiSFS` flag when running the program. See the [fastsimcoal2 manual](http://cmpg.unibe.ch/software/fastsimcoal2/) for further details. The `.obs` file can be generated with `Arlequin`, `angsd`, `easySFS` (as in this [tutorial](https://speciationgenomics.github.io/easysfs/)) or several other tools.

### Template file

The template file describes the demographic model and the parameters of interest. Values that should be estimated (i.e. model parameters such as migration rate, splitting time etc) are given as different keywords. It is best practise to use capital letters for parameter names and it is important to avoid any names that are part of another parameter name or a function (e.g. `log`, `min`, `FREQ`).

As fastsimcoal2 is a coalescent simulator, all models need to be specified **backwards in time**. This means that the most recent event is specified first. The template file is best produced by modifying an example `tpl` file in a text editor.

In this tutorial, we will first write a model of two species that diverged in the face of gene flow and then evolved complete reproductive isolation. Unfortunately, we do not have a good estimate for the mutation rate, but we have an estimate for the (maximum possible) splitting time. So here we will set the split between the populations to 6000 generations.

**Question:** Why do we need to fix a parameter (e.g. splitting time) if we do not have a reliable mutation rate estimate?

### Estimation file

All keywords introduced in the template file need to be defined in the estimation file. For each keyword, the parameter distribution (uniform or log-uniform) and search range (min and max) are given on a single line. Each parameter can be an integer or a float, as specified by a first indicator variable.

Lastly, complex parameters can be used to compute parameter values with simple operations such as computing the ratio between two simple parameters or specifying a parameter as the minimum of two parameters. In addition, for each simple or complex parameter, you need to specify if the parameter value should be written into an output file or not.

As with the template file, the estimation file is best produced by modifying an example `est` file in a text editor.

## Running fastsimcoal2

Once we have all input files ready, it is time to run `fastsimcoal2`. In addition to the input files, we need to specify how many simulations and iterations `fastsimcoal2` should perform, when to stop and how many threads (i.e. CPUs) can be used in parallel.

Let's run `fastsimcoal2` with our model of two populations. First we make a `fastsimcoal2` directory and within that a directory, an additional one for our model of `early_geneflow`:

``` shell
cd ~
mkdir fastsimcoal2
cd fastsimcoal2
mkdir early_geneflow
```

Next, we will move inside this directory and copy over the files we need

``` shell
cd ~/fastsimcoal2/early_geneflow
cp /resources/riverlandsea/exercise_data/fastsimcoal2/early_geneflow/* .
```

This should mean we have our observed SFS, our template file and our estimation file. We are now ready to run `fastsimcoal2` - we will set a `PREFIX` variable to make our command here a bit easier to write. This will help downstream too!

``` shell
PREFIX="early_geneflow"
fastsimcoal2 -t ${PREFIX}.tpl -e ${PREFIX}.est -m -0 -C 10 -n 10000 -L 40 -s 0 -M
```

This command runs `fastsimcoal2` using a MAF (`-m`) while ignoring monomorphic sites (`-0`) and SFS entries with less than 10 SNPs (`-C`). This means that entries with less than 10 SNPs are pooled together. This option is useful when there are many entries in the observed SFS with few SNPs and with a limited number of SNPS to avoid overfitting.

`fastsimcoal2` will also perform (`-n`) 10,000 coalescent simulations to approximate the expected SFS in each cycle and will run (`-L`) 40 optimization (ECM) cycles to estimate the parameters. The number of ECM cycles should be at least 20, better between 50 and 100. The number of coalescent simulations should ideally be something between 200,000 and 1,000,000 but to make it faster, we are now only running 10,000 simulations. We also specify (`-M`) that we want to perform parameter estimation. With `-s 0`, we can tell `fastsimcoal2` to output SNPs.

Once `fastsimcoal2` is finished, we can have a look at the output files. It produced a folder called `${PREFIX}` which contains a number of new files. The most relevant files are the following:

-   `${PREFIX}.bestlhoods:` a file with the Maximum likelihood estimates for each parameter specified "output" in the `est` file and the model likelihoods.
-   `${PREFIX}._jointMAFpop1_0.txt`: a file with the expected SFS obtained with the parameters that maximized the likelihood during optimization. This is needed to visually check the fit of the expected SFS to the observed SFS. This file has the same suffix as the observed SFS provided.
-   `${PREFIX}.simparam`: a file with an example of the settings to run the simulations. This is useful to check when you have errors. Many times errors in specification of models can be detected in this file.
-   `${PREFIX}_maxL.par`: The model specification file with the best parameter estimates. It is basically the tpl file with the keywords replaced by estimated values. This file is useful if you want to simulate data under the best model using Arlequin.

Note, the bestlhoods file contains two different likelihoods: `MaxObsLhood` is the maximum possible value for the likelihood if there was a perfect fit of the expected to the observed SFS, i.e. if the expected SFS was the relative observed SFS. `MaxEstLhood` is the maximum likelihood estimated according to the model parameters. It is obtained by using the observed SFS as the expected SFS when computing the likelihood, i.e., returning the value of the likelihood if there was a perfect fit between the expected and observed SFS.

The better the fit, the smaller the difference between `MaxObsLhood` and `MaxEstLhood`.

### Finding the best parameter estimates

`fastsimcoal2` should not just be run once because it might not find the global optimum of the best combination of parameter estimates right away. It is better to run it 100 times or more. Of these runs, we then go on to select the one with the highest likelihood which is the run with the best fitting parameter estimates for this model.

Due to time constraints, we will only run this model 5 times and we will do so within a `for` loop. Note, that here we have added the flag `-q` for "quiet" which reduces the amount of information `fastsimcoal2` writes to stdout. Pay attention the `cd` commands here as they ensure that the analyses are done in the correct directory.

``` shell
 for i in {1..5}
 do
   mkdir run$i
   cp ${PREFIX}.tpl ${PREFIX}.est ${PREFIX}_jointMAFpop1_0.obs run$i"/"
   cd run$i
   fastsimcoal2 -t ${PREFIX}.tpl -e ${PREFIX}.est -m -0 -C 10 -n 100 -L 40 -s0 -M -q
   cd ..
 done
```

To find the best run, i.e. the run with the highest likelihood, or better the smallest difference between the maximum possible likelihood (`MaxObsLhood`) and the obtained likelihood (`MaxEstLhood`), we can check the `.bestlhoods` files.

``` shell
cat run{1..5}/${PREFIX}/${PREFIX}.bestlhoods | grep -v MaxObsLhood | awk '{print NR,$8}' | sort -k 2
```

Note that `NR` in awk prints out the line number which here corresponds to the run number. `$8` is the `MaxEstLhood` column and thus the likelihood we want to compare across different runs.

\[Joana Meier\](https://www.sanger.ac.uk/person/meier-joana/ has written a [script](https://github.com/speciationgenomics/scripts/raw/master/fsc-selectbestrun.sh) for you that automatically extracts the files of the best run and copies them into a new folder which it calls `bestrun`. A copy of that script is available in the course resources directory so we will copy it to our run folder directory and run it like so:

Just run it in the directory where all the folders run are located and run it:

``` shell
cd ~/fastsimcoal2/early_geneflow
cp /resources/riverlandsea/exercise_data/fastsimcoal2/fsc-selectbestrun.sh .
./fsc-selectbestrun.sh
```

Now modify the `$PREFIX.tpl` and `$PREFIX.est` files to specify different models and also rename the SFS to `${PREFIX}_jointDAFpop1_0.txt`. Then run `fastsimcoal2`for all models to see which one shows the best fit to the observed SFS.

![](/images/modeling/models.png)

<details>

<summary>Click here to see a possible solution.</summary>

<p>Early Geneflow (this is the model expalined in the example, gene flow right after the split and then no gene flow anymore, e.g. speciation with gene flow and then no gene flow anymore as reproductive isolation becomes strong): <a href="https://github.com/speciationgenomics/data/blob/master/early_geneflow.tpl"> tpl</a> <a href="https://github.com/speciationgenomics/data/blob/master/early_geneflow.est"> est</a> <br/> <br/> No geneflow: <a href="https://github.com/speciationgenomics/data/blob/master/no_geneflow.tpl"> tpl</a> <a href="https://github.com/speciationgenomics/data/blob/master/no_geneflow.est"> est</a> <br/> <br/> Recent geneflow (no gene flow intially after the split but gene flow in recent times, e.g. secondary contact scenario) <a href="https://github.com/speciationgenomics/data/blob/master/recent_geneflow.tpl"> tpl</a> <a href="https://github.com/speciationgenomics/data/blob/master/recent_geneflow.est"> est</a> <br/> <br/> Different gene flow matrices (higher or lower gene flow right after the split than recently, e.g. initially higher gene flow then lower gene flow as reproductive isolation accumulates): <a href="https://github.com/speciationgenomics/data/blob/master/diff_geneflow.tpl"> tpl</a> <a href="https://github.com/speciationgenomics/data/blob/master/diff_geneflow.est"> est</a> <br/> <br/> Constant gene flow (same gene flow strengths since the split until now): <a href="https://github.com/speciationgenomics/data/blob/master/ongoing_geneflow.tpl"> tpl</a> <a href="https://github.com/speciationgenomics/data/blob/master/ongoing_geneflow.est"> est</a></p>

</details>

### Model comparison with AIC

In order to find the best model, the likelihoods of the best run of each model should be compared. Comparing raw likelihoods is problematic, because a model with more parameters will always tend to result in a better fit to the data. Therefore, the [Akaike information criterium](https://en.wikipedia.org/wiki/Akaike_information_criterion) or AIC is typically calculated to determine if the models differ in their likelihoods accounting for the number of parameters in each model. To perform, this, we can use a [script](https://github.com/speciationgenomics/scripts/blob/master/calculateAIC.sh) that is mostly based on R code by [Vitor Sousa](http://ce3c.ciencias.ulisboa.pt/member/vitorsousa). We will move into our `bestrun` directory and run this script

``` shell
cd bestrun/
cp /resources/riverlandsea/exercise_data/fastsimcoal2/calculateAIC.sh .
./calculateAIC.sh early_geneflow
```

This script generates a file `${PREFIX}.AIC` which contains the delta likelihood and the AIC value for that run.

### Visualize the model fit

To visualize the fit of the simulated SFS to the data, we can use an `R` script that David Marques wrote - `SFStools.r` - which you can download [here](https://github.com/marqueda/SFS-scripts/).

To visualize the model with the best parameter estimates, we can use one of Joana Meier's R scripts - [plotModel.r](https://github.com/speciationgenomics/scripts/blob/master/plotModel.r). There are also other options [here](http://cmpg.unibe.ch/software/fastsimcoal27/additionalScripts.html). However for the ease of this practical, both are available in the resources directory. We will copy them over and then run them.

``` shell
cp /resources/riverlandsea/exercise_data/fastsimcoal2/*.r .
Rscript SFStools.r -t print2D -i early_geneflow
Rscript plotModel.r -p early_geneflow -l NyerMak,PundMak
```

Now, we will download the PDFs genertated and take a look at them.

### Model comparison with Likelihood distributions

One drawback of the composite likelihoods in model tests based on AIC is that it can overestimate the support for the most likely model if the SNPs are not independent (here they are not LD-pruned). Another way to infer if the models are really different and do not just differ because of stochasticity in the likelihood approximation, is to get likelihood distributions for each model. This is done by running each model with the best parameter values multple times (ideally about 100 times). The likelihoods will differ because `fastsimcoal2` does not compute the likelihood but rather approximates it with simulations. If the ranges of likelihoods of two models overlap, it means that they do not differ significantly, i.e. provide an equally good fit to the observed data.

Let's recompute the likelihood for the best run for the `early_geneflow` model.

``` shell

PREFIX="early_geneflow"
cd ~/fastsimcoal/$PREFIX/bestrun

# create temporary obs file with name _maxL_MSFS.obs
cp ${PREFIX}_jointMAFpop1_0.obs ${PREFIX}_maxL_jointMAFpop1_0.obs

# Run fastsimcoal 20 times (in reality better 100 times) to get the likelihood of the observed SFS under the best parameter values with 1 mio simulated SFS.
for i in {1..20}
do
 fastsimcoal2 -i ${PREFIX}_maxL.par -n1000000 -m -q -0
 # Fastsimcoal will generate a new folder called ${model}_maxL and write files in there

 # collect the lhood values (Note that >> appends to the file, whereas > would overwrite it)
 sed -n '2,3p' ${PREFIX}_maxL/${PREFIX}_maxL.lhoods  >> ${PREFIX}.lhoods

 # delete the folder with results
 rm -r ${PREFIX}_maxL/
done
```

We would now repeat this for different models: ongoing gene flow, a model without gene flow, a model of secondary contact (recent gene flow) and a model with different amounts of gene flow in the past and in recent times.

Due to time constraints, we will give you the results of these models. They are in the same format as the folder we generated for the model with early gene flow. Load them into your own directory. Note, the `-r` flag stands for "recursive" and allows to also copy directories and their contents.

``` shell
cd ~/fastsimcoal2/
cp -r /resources/riverlandsea/exercise_data/fastsimcoal2/extramodels
```

Now, let's access the R Studio sderver and plot the likelihoods.

``` r


# Read in the likelihoods
early_geneflow<-scan("early_geneflow.lhoods")
ongoing_geneflow<-scan("ongoing_geneflow.lhoods")
diff_geneflow<-scan("diff_geneflow.lhoods")
recent_geneflow<-scan("recent_geneflow.lhoods")
no_geneflow<-scan("no_geneflow.lhoods")

# Plot the likelihoods

par(mfrow=c(1,1))
boxplot(range = 0,diff_geneflow,recent_geneflow,early_geneflow,ongoing_geneflow,
        no_geneflow, ylab="Likelihood",xaxt="n")
axis(side=1,at=1:5, labels=c("early+recent","recent","early","constant","no"))
```

Similarly, we should compare the AIC values and see if AIC suggests that the second-best model is significantly less good than the best model. Given that we already used the calculateAIC.r script to compute AIC values, this can easily be done.

``` shell
for i in */bestrun/*AIC
do
echo -e `basename $i`"\t"`tail -n $i` >> allmodels.AIC
done
```

And we're done for this tutorial! It is worth noting that linkage among SNPs can actually distort the AIC and likelihood calculations. As a result it is best practice to perform block-bootstrapping to account for this. We don't have time to implement that here but you can refer to the [original version](https://speciationgenomics.github.io/fastsimcoal2/) of this tutorial which does have a guide for how to achieve this.
